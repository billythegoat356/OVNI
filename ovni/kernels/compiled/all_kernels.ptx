//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_75
.address_size 64

	// .globl	bilinear

.visible .entry bilinear(
	.param .u64 bilinear_param_0,
	.param .u64 bilinear_param_1,
	.param .u64 bilinear_param_2,
	.param .u64 bilinear_param_3,
	.param .u64 bilinear_param_4,
	.param .u32 bilinear_param_5,
	.param .u32 bilinear_param_6,
	.param .f32 bilinear_param_7,
	.param .f32 bilinear_param_8
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<18>;
	.reg .f32 	%f<58>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd4, [bilinear_param_0];
	ld.param.u64 	%rd5, [bilinear_param_1];
	ld.param.u64 	%rd6, [bilinear_param_2];
	ld.param.u64 	%rd7, [bilinear_param_3];
	ld.param.u64 	%rd8, [bilinear_param_4];
	ld.param.u32 	%r3, [bilinear_param_5];
	ld.param.u32 	%r4, [bilinear_param_6];
	ld.param.f32 	%f15, [bilinear_param_7];
	ld.param.f32 	%f16, [bilinear_param_8];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_6;

	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mul.lo.s32 	%r12, %r11, 3;
	cvt.rzi.s32.f32 	%r13, %f15;
	cvt.rn.f32.s32 	%f17, %r13;
	setp.eq.f32 	%p4, %f17, %f15;
	cvt.s64.s32 	%rd1, %r12;
	cvta.to.global.u64 	%rd9, %rd5;
	add.s64 	%rd2, %rd9, %rd1;
	ld.global.u8 	%rs1, [%rd2];
	cvt.rn.f32.u16 	%f1, %rs1;
	cvta.to.global.u64 	%rd10, %rd7;
	add.s64 	%rd3, %rd10, %rd1;
	@%p4 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_2;

$L__BB0_4:
	mov.f32 	%f45, 0f3F800000;
	sub.f32 	%f46, %f45, %f16;
	ld.global.u8 	%rs13, [%rd3];
	cvt.rn.f32.u16 	%f47, %rs13;
	mul.f32 	%f48, %f47, %f16;
	fma.rn.f32 	%f55, %f46, %f1, %f48;
	ld.global.u8 	%rs14, [%rd2+1];
	cvt.rn.f32.u16 	%f49, %rs14;
	ld.global.u8 	%rs15, [%rd3+1];
	cvt.rn.f32.u16 	%f50, %rs15;
	mul.f32 	%f51, %f50, %f16;
	fma.rn.f32 	%f56, %f46, %f49, %f51;
	ld.global.u8 	%rs16, [%rd2+2];
	cvt.rn.f32.u16 	%f52, %rs16;
	ld.global.u8 	%rs17, [%rd3+2];
	cvt.rn.f32.u16 	%f53, %rs17;
	mul.f32 	%f54, %f53, %f16;
	fma.rn.f32 	%f57, %f46, %f52, %f54;
	bra.uni 	$L__BB0_5;

$L__BB0_2:
	mov.f32 	%f18, 0f3F800000;
	sub.f32 	%f2, %f18, %f15;
	cvta.to.global.u64 	%rd11, %rd6;
	add.s64 	%rd12, %rd11, %rd1;
	ld.global.u8 	%rs2, [%rd12];
	cvt.rn.f32.u16 	%f19, %rs2;
	mul.f32 	%f20, %f19, %f15;
	fma.rn.f32 	%f55, %f2, %f1, %f20;
	ld.global.u8 	%rs3, [%rd2+1];
	cvt.rn.f32.u16 	%f21, %rs3;
	ld.global.u8 	%rs4, [%rd12+1];
	cvt.rn.f32.u16 	%f22, %rs4;
	mul.f32 	%f23, %f22, %f15;
	fma.rn.f32 	%f56, %f2, %f21, %f23;
	ld.global.u8 	%rs5, [%rd2+2];
	cvt.rn.f32.u16 	%f24, %rs5;
	ld.global.u8 	%rs6, [%rd12+2];
	cvt.rn.f32.u16 	%f25, %rs6;
	mul.f32 	%f26, %f25, %f15;
	fma.rn.f32 	%f57, %f2, %f24, %f26;
	cvt.rzi.s32.f32 	%r14, %f16;
	cvt.rn.f32.s32 	%f27, %r14;
	setp.eq.f32 	%p5, %f27, %f16;
	@%p5 bra 	$L__BB0_5;

	ld.global.u8 	%rs7, [%rd3];
	cvt.rn.f32.u16 	%f28, %rs7;
	cvta.to.global.u64 	%rd13, %rd8;
	add.s64 	%rd14, %rd13, %rd1;
	ld.global.u8 	%rs8, [%rd14];
	cvt.rn.f32.u16 	%f29, %rs8;
	mul.f32 	%f30, %f29, %f15;
	fma.rn.f32 	%f31, %f2, %f28, %f30;
	ld.global.u8 	%rs9, [%rd3+1];
	cvt.rn.f32.u16 	%f32, %rs9;
	ld.global.u8 	%rs10, [%rd14+1];
	cvt.rn.f32.u16 	%f33, %rs10;
	mul.f32 	%f34, %f33, %f15;
	fma.rn.f32 	%f35, %f2, %f32, %f34;
	ld.global.u8 	%rs11, [%rd3+2];
	cvt.rn.f32.u16 	%f36, %rs11;
	ld.global.u8 	%rs12, [%rd14+2];
	cvt.rn.f32.u16 	%f37, %rs12;
	mul.f32 	%f38, %f37, %f15;
	fma.rn.f32 	%f39, %f2, %f36, %f38;
	sub.f32 	%f41, %f18, %f16;
	mul.f32 	%f42, %f31, %f16;
	fma.rn.f32 	%f55, %f41, %f55, %f42;
	mul.f32 	%f43, %f35, %f16;
	fma.rn.f32 	%f56, %f41, %f56, %f43;
	mul.f32 	%f44, %f39, %f16;
	fma.rn.f32 	%f57, %f41, %f57, %f44;

$L__BB0_5:
	cvt.rzi.u32.f32 	%r15, %f55;
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd1;
	st.global.u8 	[%rd16], %r15;
	cvt.rzi.u32.f32 	%r16, %f56;
	st.global.u8 	[%rd16+1], %r16;
	cvt.rzi.u32.f32 	%r17, %f57;
	st.global.u8 	[%rd16+2], %r17;

$L__BB0_6:
	ret;

}
	// .globl	nv12_to_rgb
.visible .entry nv12_to_rgb(
	.param .u64 nv12_to_rgb_param_0,
	.param .u64 nv12_to_rgb_param_1,
	.param .u64 nv12_to_rgb_param_2,
	.param .u32 nv12_to_rgb_param_3,
	.param .u32 nv12_to_rgb_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [nv12_to_rgb_param_0];
	ld.param.u64 	%rd2, [nv12_to_rgb_param_1];
	ld.param.u64 	%rd3, [nv12_to_rgb_param_2];
	ld.param.u32 	%r3, [nv12_to_rgb_param_3];
	ld.param.u32 	%r4, [nv12_to_rgb_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd4, %rd1;
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	shr.u32 	%r12, %r2, 31;
	add.s32 	%r13, %r2, %r12;
	shr.s32 	%r14, %r13, 1;
	shr.u32 	%r15, %r1, 31;
	add.s32 	%r16, %r1, %r15;
	and.b32  	%r17, %r16, -2;
	mad.lo.s32 	%r18, %r14, %r3, %r17;
	cvt.s64.s32 	%rd5, %r11;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u8 	%rs1, [%rd6];
	cvt.rn.f32.u16 	%f1, %rs1;
	cvt.s64.s32 	%rd7, %r18;
	cvta.to.global.u64 	%rd8, %rd2;
	add.s64 	%rd9, %rd8, %rd7;
	ld.global.u8 	%rs2, [%rd9];
	cvt.rn.f32.u16 	%f2, %rs2;
	add.f32 	%f3, %f2, 0fC3000000;
	ld.global.u8 	%rs3, [%rd9+1];
	cvt.rn.f32.u16 	%f4, %rs3;
	add.f32 	%f5, %f4, 0fC3000000;
	fma.rn.f32 	%f6, %f5, 0f3FC9930C, %f1;
	fma.rn.f32 	%f7, %f3, 0fBE3FCB92, %f1;
	fma.rn.f32 	%f8, %f5, 0fBEEFAACE, %f7;
	fma.rn.f32 	%f9, %f3, 0f3FED844D, %f1;
	mov.f32 	%f10, 0f00000000;
	max.f32 	%f11, %f6, %f10;
	mov.f32 	%f12, 0f437F0000;
	min.f32 	%f13, %f11, %f12;
	max.f32 	%f14, %f8, %f10;
	min.f32 	%f15, %f14, %f12;
	max.f32 	%f16, %f9, %f10;
	min.f32 	%f17, %f16, %f12;
	mul.lo.s32 	%r19, %r11, 3;
	cvt.rzi.u32.f32 	%r20, %f13;
	cvt.s64.s32 	%rd10, %r19;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd10;
	st.global.u8 	[%rd12], %r20;
	cvt.rzi.u32.f32 	%r21, %f15;
	st.global.u8 	[%rd12+1], %r21;
	cvt.rzi.u32.f32 	%r22, %f17;
	st.global.u8 	[%rd12+2], %r22;

$L__BB1_2:
	ret;

}
	// .globl	rgb_to_nv12
.visible .entry rgb_to_nv12(
	.param .u64 rgb_to_nv12_param_0,
	.param .u64 rgb_to_nv12_param_1,
	.param .u64 rgb_to_nv12_param_2,
	.param .u32 rgb_to_nv12_param_3,
	.param .u32 rgb_to_nv12_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd1, [rgb_to_nv12_param_0];
	ld.param.u64 	%rd2, [rgb_to_nv12_param_1];
	ld.param.u64 	%rd3, [rgb_to_nv12_param_2];
	ld.param.u32 	%r3, [rgb_to_nv12_param_3];
	ld.param.u32 	%r4, [rgb_to_nv12_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB2_3;

	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mul.lo.s32 	%r12, %r11, 3;
	cvt.s64.s32 	%rd5, %r12;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.u8 	%rs1, [%rd7];
	cvt.rn.f32.u16 	%f3, %rs1;
	ld.global.u8 	%rs2, [%rd7+1];
	cvt.rn.f32.u16 	%f4, %rs2;
	ld.global.u8 	%rs3, [%rd7+2];
	cvt.rn.f32.u16 	%f5, %rs3;
	mul.f32 	%f6, %f4, 0f3F371759;
	fma.rn.f32 	%f7, %f3, 0f3E59B3D0, %f6;
	fma.rn.f32 	%f8, %f5, 0f3D93DD98, %f7;
	sub.f32 	%f1, %f5, %f8;
	sub.f32 	%f2, %f3, %f8;
	mov.f32 	%f9, 0f00000000;
	max.f32 	%f10, %f8, %f9;
	mov.f32 	%f11, 0f437F0000;
	min.f32 	%f12, %f10, %f11;
	cvt.rzi.u32.f32 	%r13, %f12;
	cvt.s64.s32 	%rd8, %r11;
	add.s64 	%rd9, %rd4, %rd8;
	st.global.u8 	[%rd9], %r13;
	and.b32  	%r14, %r2, 1;
	setp.eq.b32 	%p4, %r14, 1;
	and.b32  	%r15, %r1, 1;
	setp.eq.b32 	%p5, %r15, 1;
	or.pred  	%p6, %p5, %p4;
	mov.pred 	%p7, 0;
	xor.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB2_3;

	div.rn.f32 	%f13, %f2, 0f3FC9930C;
	add.f32 	%f14, %f13, 0f43000000;
	max.f32 	%f16, %f14, %f9;
	min.f32 	%f18, %f16, %f11;
	div.rn.f32 	%f19, %f1, 0f3FED844D;
	add.f32 	%f20, %f19, 0f43000000;
	max.f32 	%f21, %f20, %f9;
	min.f32 	%f22, %f21, %f11;
	shr.u32 	%r16, %r2, 31;
	add.s32 	%r17, %r2, %r16;
	shr.s32 	%r18, %r17, 1;
	mad.lo.s32 	%r19, %r18, %r3, %r1;
	cvt.rzi.u32.f32 	%r20, %f22;
	cvt.s64.s32 	%rd10, %r19;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd10;
	st.global.u8 	[%rd12], %r20;
	cvt.rzi.u32.f32 	%r21, %f18;
	st.global.u8 	[%rd12+1], %r21;

$L__BB2_3:
	ret;

}
	// .globl	scale_translate
.visible .entry scale_translate(
	.param .u64 scale_translate_param_0,
	.param .u32 scale_translate_param_1,
	.param .u32 scale_translate_param_2,
	.param .u64 scale_translate_param_3,
	.param .u32 scale_translate_param_4,
	.param .u32 scale_translate_param_5,
	.param .f32 scale_translate_param_6,
	.param .u32 scale_translate_param_7,
	.param .u32 scale_translate_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<63>;
	.reg .b32 	%r<49>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [scale_translate_param_0];
	ld.param.u32 	%r4, [scale_translate_param_1];
	ld.param.u32 	%r5, [scale_translate_param_2];
	ld.param.u64 	%rd2, [scale_translate_param_3];
	ld.param.u32 	%r6, [scale_translate_param_4];
	ld.param.u32 	%r9, [scale_translate_param_5];
	ld.param.f32 	%f5, [scale_translate_param_6];
	ld.param.u32 	%r7, [scale_translate_param_7];
	ld.param.u32 	%r8, [scale_translate_param_8];
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r11, %r10, %r12;
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r2, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r9;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_4;

	mad.lo.s32 	%r3, %r2, %r6, %r1;
	sub.s32 	%r16, %r1, %r7;
	cvt.rn.f32.s32 	%f6, %r16;
	div.rn.f32 	%f1, %f6, %f5;
	sub.s32 	%r17, %r2, %r8;
	cvt.rn.f32.s32 	%f7, %r17;
	div.rn.f32 	%f2, %f7, %f5;
	setp.lt.f32 	%p4, %f1, 0f00000000;
	cvt.rn.f32.s32 	%f3, %r4;
	setp.gt.f32 	%p5, %f1, %f3;
	or.pred  	%p6, %p4, %p5;
	setp.lt.f32 	%p7, %f2, 0f00000000;
	or.pred  	%p8, %p6, %p7;
	cvt.rn.f32.s32 	%f4, %r5;
	setp.gt.f32 	%p9, %f2, %f4;
	or.pred  	%p10, %p9, %p8;
	mov.u16 	%rs22, 0;
	mov.u16 	%rs23, %rs22;
	mov.u16 	%rs24, %rs22;
	@%p10 bra 	$L__BB3_3;

	cvta.to.global.u64 	%rd3, %rd1;
	add.f32 	%f8, %f3, 0fBF800000;
	min.f32 	%f9, %f1, %f8;
	mov.f32 	%f10, 0f00000000;
	max.f32 	%f11, %f10, %f9;
	add.f32 	%f12, %f4, 0fBF800000;
	min.f32 	%f13, %f2, %f12;
	max.f32 	%f14, %f10, %f13;
	cvt.rmi.f32.f32 	%f15, %f11;
	cvt.rzi.s32.f32 	%r18, %f15;
	add.s32 	%r19, %r18, 1;
	add.s32 	%r20, %r4, -1;
	min.s32 	%r21, %r19, %r20;
	cvt.rmi.f32.f32 	%f16, %f14;
	cvt.rzi.s32.f32 	%r22, %f16;
	add.s32 	%r23, %r22, 1;
	add.s32 	%r24, %r5, -1;
	min.s32 	%r25, %r23, %r24;
	cvt.rn.f32.s32 	%f17, %r18;
	sub.f32 	%f18, %f11, %f17;
	mul.lo.s32 	%r26, %r22, %r4;
	add.s32 	%r27, %r26, %r18;
	mul.lo.s32 	%r28, %r27, 3;
	add.s32 	%r29, %r26, %r21;
	mul.lo.s32 	%r30, %r29, 3;
	cvt.s64.s32 	%rd4, %r28;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u8 	%rs10, [%rd5];
	cvt.rn.f32.u16 	%f19, %rs10;
	mov.f32 	%f20, 0f3F800000;
	sub.f32 	%f21, %f20, %f18;
	cvt.s64.s32 	%rd6, %r30;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.u8 	%rs11, [%rd7];
	cvt.rn.f32.u16 	%f22, %rs11;
	mul.f32 	%f23, %f18, %f22;
	fma.rn.f32 	%f24, %f21, %f19, %f23;
	ld.global.u8 	%rs12, [%rd5+1];
	cvt.rn.f32.u16 	%f25, %rs12;
	ld.global.u8 	%rs13, [%rd7+1];
	cvt.rn.f32.u16 	%f26, %rs13;
	mul.f32 	%f27, %f18, %f26;
	fma.rn.f32 	%f28, %f21, %f25, %f27;
	ld.global.u8 	%rs14, [%rd5+2];
	cvt.rn.f32.u16 	%f29, %rs14;
	ld.global.u8 	%rs15, [%rd7+2];
	cvt.rn.f32.u16 	%f30, %rs15;
	mul.f32 	%f31, %f18, %f30;
	fma.rn.f32 	%f32, %f21, %f29, %f31;
	mul.lo.s32 	%r31, %r25, %r4;
	add.s32 	%r32, %r31, %r18;
	mul.lo.s32 	%r33, %r32, 3;
	add.s32 	%r34, %r31, %r21;
	mul.lo.s32 	%r35, %r34, 3;
	cvt.s64.s32 	%rd8, %r33;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.u8 	%rs16, [%rd9];
	cvt.rn.f32.u16 	%f33, %rs16;
	cvt.s64.s32 	%rd10, %r35;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.u8 	%rs17, [%rd11];
	cvt.rn.f32.u16 	%f34, %rs17;
	mul.f32 	%f35, %f18, %f34;
	fma.rn.f32 	%f36, %f21, %f33, %f35;
	ld.global.u8 	%rs18, [%rd9+1];
	cvt.rn.f32.u16 	%f37, %rs18;
	ld.global.u8 	%rs19, [%rd11+1];
	cvt.rn.f32.u16 	%f38, %rs19;
	mul.f32 	%f39, %f18, %f38;
	fma.rn.f32 	%f40, %f21, %f37, %f39;
	ld.global.u8 	%rs20, [%rd9+2];
	cvt.rn.f32.u16 	%f41, %rs20;
	ld.global.u8 	%rs21, [%rd11+2];
	cvt.rn.f32.u16 	%f42, %rs21;
	mul.f32 	%f43, %f18, %f42;
	fma.rn.f32 	%f44, %f21, %f41, %f43;
	cvt.rn.f32.s32 	%f45, %r22;
	sub.f32 	%f46, %f14, %f45;
	sub.f32 	%f47, %f20, %f46;
	mul.f32 	%f48, %f46, %f36;
	fma.rn.f32 	%f49, %f47, %f24, %f48;
	mov.b32 	%r36, %f49;
	and.b32  	%r37, %r36, -2147483648;
	or.b32  	%r38, %r37, 1056964608;
	mov.b32 	%f50, %r38;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.u32.f32 	%r39, %f52;
	cvt.u16.u32 	%rs23, %r39;
	mul.f32 	%f53, %f46, %f40;
	fma.rn.f32 	%f54, %f47, %f28, %f53;
	mov.b32 	%r40, %f54;
	and.b32  	%r41, %r40, -2147483648;
	or.b32  	%r42, %r41, 1056964608;
	mov.b32 	%f55, %r42;
	add.rz.f32 	%f56, %f54, %f55;
	cvt.rzi.f32.f32 	%f57, %f56;
	cvt.rzi.u32.f32 	%r43, %f57;
	cvt.u16.u32 	%rs22, %r43;
	mul.f32 	%f58, %f46, %f44;
	fma.rn.f32 	%f59, %f47, %f32, %f58;
	mov.b32 	%r44, %f59;
	and.b32  	%r45, %r44, -2147483648;
	or.b32  	%r46, %r45, 1056964608;
	mov.b32 	%f60, %r46;
	add.rz.f32 	%f61, %f59, %f60;
	cvt.rzi.f32.f32 	%f62, %f61;
	cvt.rzi.u32.f32 	%r47, %f62;
	cvt.u16.u32 	%rs24, %r47;

$L__BB3_3:
	mul.lo.s32 	%r48, %r3, 3;
	cvt.s64.s32 	%rd12, %r48;
	cvta.to.global.u64 	%rd13, %rd2;
	add.s64 	%rd14, %rd13, %rd12;
	st.global.u8 	[%rd14], %rs23;
	st.global.u8 	[%rd14+1], %rs22;
	st.global.u8 	[%rd14+2], %rs24;

$L__BB3_4:
	ret;

}
	// .globl	resize
.visible .entry resize(
	.param .u64 resize_param_0,
	.param .u32 resize_param_1,
	.param .u32 resize_param_2,
	.param .u64 resize_param_3,
	.param .u32 resize_param_4,
	.param .u32 resize_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [resize_param_0];
	ld.param.u32 	%r4, [resize_param_1];
	ld.param.u32 	%r5, [resize_param_2];
	ld.param.u64 	%rd2, [resize_param_3];
	ld.param.u32 	%r6, [resize_param_4];
	ld.param.u32 	%r7, [resize_param_5];
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r1, %r9, %r8, %r10;
	mov.u32 	%r11, %ntid.y;
	mov.u32 	%r12, %ctaid.y;
	mov.u32 	%r13, %tid.y;
	mad.lo.s32 	%r2, %r12, %r11, %r13;
	setp.ge.s32 	%p1, %r1, %r6;
	setp.ge.s32 	%p2, %r2, %r7;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB4_4;

	mad.lo.s32 	%r3, %r2, %r6, %r1;
	cvt.rn.f32.s32 	%f5, %r6;
	cvt.rn.f32.s32 	%f1, %r4;
	div.rn.f32 	%f6, %f1, %f5;
	cvt.rn.f32.s32 	%f7, %r1;
	mul.f32 	%f2, %f6, %f7;
	cvt.rn.f32.s32 	%f8, %r7;
	cvt.rn.f32.s32 	%f3, %r5;
	div.rn.f32 	%f9, %f3, %f8;
	cvt.rn.f32.s32 	%f10, %r2;
	mul.f32 	%f4, %f9, %f10;
	setp.lt.f32 	%p4, %f2, 0f00000000;
	setp.gt.f32 	%p5, %f2, %f1;
	or.pred  	%p6, %p4, %p5;
	setp.lt.f32 	%p7, %f4, 0f00000000;
	or.pred  	%p8, %p6, %p7;
	setp.gt.f32 	%p9, %f4, %f3;
	or.pred  	%p10, %p9, %p8;
	mov.u16 	%rs22, 0;
	mov.u16 	%rs23, %rs22;
	mov.u16 	%rs24, %rs22;
	@%p10 bra 	$L__BB4_3;

	cvta.to.global.u64 	%rd3, %rd1;
	add.f32 	%f11, %f1, 0fBF800000;
	min.f32 	%f12, %f2, %f11;
	mov.f32 	%f13, 0f00000000;
	max.f32 	%f14, %f13, %f12;
	add.f32 	%f15, %f3, 0fBF800000;
	min.f32 	%f16, %f4, %f15;
	max.f32 	%f17, %f13, %f16;
	cvt.rmi.f32.f32 	%f18, %f14;
	cvt.rzi.s32.f32 	%r14, %f18;
	add.s32 	%r15, %r14, 1;
	add.s32 	%r16, %r4, -1;
	min.s32 	%r17, %r15, %r16;
	cvt.rmi.f32.f32 	%f19, %f17;
	cvt.rzi.s32.f32 	%r18, %f19;
	add.s32 	%r19, %r18, 1;
	add.s32 	%r20, %r5, -1;
	min.s32 	%r21, %r19, %r20;
	cvt.rn.f32.s32 	%f20, %r14;
	sub.f32 	%f21, %f14, %f20;
	mul.lo.s32 	%r22, %r18, %r4;
	add.s32 	%r23, %r22, %r14;
	mul.lo.s32 	%r24, %r23, 3;
	add.s32 	%r25, %r22, %r17;
	mul.lo.s32 	%r26, %r25, 3;
	cvt.s64.s32 	%rd4, %r24;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u8 	%rs10, [%rd5];
	cvt.rn.f32.u16 	%f22, %rs10;
	mov.f32 	%f23, 0f3F800000;
	sub.f32 	%f24, %f23, %f21;
	cvt.s64.s32 	%rd6, %r26;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.u8 	%rs11, [%rd7];
	cvt.rn.f32.u16 	%f25, %rs11;
	mul.f32 	%f26, %f21, %f25;
	fma.rn.f32 	%f27, %f24, %f22, %f26;
	ld.global.u8 	%rs12, [%rd5+1];
	cvt.rn.f32.u16 	%f28, %rs12;
	ld.global.u8 	%rs13, [%rd7+1];
	cvt.rn.f32.u16 	%f29, %rs13;
	mul.f32 	%f30, %f21, %f29;
	fma.rn.f32 	%f31, %f24, %f28, %f30;
	ld.global.u8 	%rs14, [%rd5+2];
	cvt.rn.f32.u16 	%f32, %rs14;
	ld.global.u8 	%rs15, [%rd7+2];
	cvt.rn.f32.u16 	%f33, %rs15;
	mul.f32 	%f34, %f21, %f33;
	fma.rn.f32 	%f35, %f24, %f32, %f34;
	mul.lo.s32 	%r27, %r21, %r4;
	add.s32 	%r28, %r27, %r14;
	mul.lo.s32 	%r29, %r28, 3;
	add.s32 	%r30, %r27, %r17;
	mul.lo.s32 	%r31, %r30, 3;
	cvt.s64.s32 	%rd8, %r29;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.u8 	%rs16, [%rd9];
	cvt.rn.f32.u16 	%f36, %rs16;
	cvt.s64.s32 	%rd10, %r31;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.u8 	%rs17, [%rd11];
	cvt.rn.f32.u16 	%f37, %rs17;
	mul.f32 	%f38, %f21, %f37;
	fma.rn.f32 	%f39, %f24, %f36, %f38;
	ld.global.u8 	%rs18, [%rd9+1];
	cvt.rn.f32.u16 	%f40, %rs18;
	ld.global.u8 	%rs19, [%rd11+1];
	cvt.rn.f32.u16 	%f41, %rs19;
	mul.f32 	%f42, %f21, %f41;
	fma.rn.f32 	%f43, %f24, %f40, %f42;
	ld.global.u8 	%rs20, [%rd9+2];
	cvt.rn.f32.u16 	%f44, %rs20;
	ld.global.u8 	%rs21, [%rd11+2];
	cvt.rn.f32.u16 	%f45, %rs21;
	mul.f32 	%f46, %f21, %f45;
	fma.rn.f32 	%f47, %f24, %f44, %f46;
	cvt.rn.f32.s32 	%f48, %r18;
	sub.f32 	%f49, %f17, %f48;
	sub.f32 	%f50, %f23, %f49;
	mul.f32 	%f51, %f49, %f39;
	fma.rn.f32 	%f52, %f50, %f27, %f51;
	mov.b32 	%r32, %f52;
	and.b32  	%r33, %r32, -2147483648;
	or.b32  	%r34, %r33, 1056964608;
	mov.b32 	%f53, %r34;
	add.rz.f32 	%f54, %f52, %f53;
	cvt.rzi.f32.f32 	%f55, %f54;
	cvt.rzi.u32.f32 	%r35, %f55;
	cvt.u16.u32 	%rs23, %r35;
	mul.f32 	%f56, %f49, %f43;
	fma.rn.f32 	%f57, %f50, %f31, %f56;
	mov.b32 	%r36, %f57;
	and.b32  	%r37, %r36, -2147483648;
	or.b32  	%r38, %r37, 1056964608;
	mov.b32 	%f58, %r38;
	add.rz.f32 	%f59, %f57, %f58;
	cvt.rzi.f32.f32 	%f60, %f59;
	cvt.rzi.u32.f32 	%r39, %f60;
	cvt.u16.u32 	%rs22, %r39;
	mul.f32 	%f61, %f49, %f47;
	fma.rn.f32 	%f62, %f50, %f35, %f61;
	mov.b32 	%r40, %f62;
	and.b32  	%r41, %r40, -2147483648;
	or.b32  	%r42, %r41, 1056964608;
	mov.b32 	%f63, %r42;
	add.rz.f32 	%f64, %f62, %f63;
	cvt.rzi.f32.f32 	%f65, %f64;
	cvt.rzi.u32.f32 	%r43, %f65;
	cvt.u16.u32 	%rs24, %r43;

$L__BB4_3:
	mul.lo.s32 	%r44, %r3, 3;
	cvt.s64.s32 	%rd12, %r44;
	cvta.to.global.u64 	%rd13, %rd2;
	add.s64 	%rd14, %rd13, %rd12;
	st.global.u8 	[%rd14], %rs23;
	st.global.u8 	[%rd14+1], %rs22;
	st.global.u8 	[%rd14+2], %rs24;

$L__BB4_4:
	ret;

}
	// .globl	overlay_opacity
.visible .entry overlay_opacity(
	.param .u64 overlay_opacity_param_0,
	.param .u64 overlay_opacity_param_1,
	.param .u32 overlay_opacity_param_2,
	.param .u32 overlay_opacity_param_3,
	.param .f32 overlay_opacity_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [overlay_opacity_param_0];
	ld.param.u64 	%rd2, [overlay_opacity_param_1];
	ld.param.u32 	%r3, [overlay_opacity_param_2];
	ld.param.u32 	%r4, [overlay_opacity_param_3];
	ld.param.f32 	%f1, [overlay_opacity_param_4];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB5_2;

	mad.lo.s32 	%r11, %r2, %r3, %r1;
	mul.lo.s32 	%r12, %r11, 3;
	cvt.s64.s32 	%rd3, %r12;
	cvta.to.global.u64 	%rd4, %rd2;
	add.s64 	%rd5, %rd4, %rd3;
	ld.global.u8 	%rs1, [%rd5];
	cvt.rn.f32.u16 	%f2, %rs1;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd7, %rd6, %rd3;
	ld.global.u8 	%rs2, [%rd7];
	cvt.rn.f32.u16 	%f3, %rs2;
	mov.f32 	%f4, 0f3F800000;
	sub.f32 	%f5, %f4, %f1;
	mul.f32 	%f6, %f5, %f3;
	fma.rn.f32 	%f7, %f2, %f1, %f6;
	cvt.rzi.u32.f32 	%r13, %f7;
	ld.global.u8 	%rs3, [%rd5+1];
	cvt.rn.f32.u16 	%f8, %rs3;
	ld.global.u8 	%rs4, [%rd7+1];
	cvt.rn.f32.u16 	%f9, %rs4;
	mul.f32 	%f10, %f5, %f9;
	fma.rn.f32 	%f11, %f8, %f1, %f10;
	cvt.rzi.u32.f32 	%r14, %f11;
	ld.global.u8 	%rs5, [%rd5+2];
	cvt.rn.f32.u16 	%f12, %rs5;
	ld.global.u8 	%rs6, [%rd7+2];
	cvt.rn.f32.u16 	%f13, %rs6;
	mul.f32 	%f14, %f5, %f13;
	fma.rn.f32 	%f15, %f12, %f1, %f14;
	cvt.rzi.u32.f32 	%r15, %f15;
	st.global.u8 	[%rd7], %r13;
	st.global.u8 	[%rd7+1], %r14;
	st.global.u8 	[%rd7+2], %r15;

$L__BB5_2:
	ret;

}
	// .globl	blend
.visible .entry blend(
	.param .u64 blend_param_0,
	.param .u64 blend_param_1,
	.param .u32 blend_param_2,
	.param .u32 blend_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [blend_param_0];
	ld.param.u64 	%rd2, [blend_param_1];
	ld.param.u32 	%r3, [blend_param_2];
	ld.param.u32 	%r4, [blend_param_3];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r2, %r9, %r8, %r10;
	setp.ge.s32 	%p1, %r1, %r3;
	setp.ge.s32 	%p2, %r2, %r4;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mad.lo.s32 	%r11, %r2, %r3, %r1;
	shl.b32 	%r12, %r11, 2;
	mul.lo.s32 	%r13, %r11, 3;
	cvt.s64.s32 	%rd5, %r12;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u8 	%rs1, [%rd6+3];
	cvt.rn.f32.u16 	%f1, %rs1;
	div.rn.f32 	%f2, %f1, 0f437F0000;
	ld.global.u8 	%rs2, [%rd6];
	cvt.rn.f32.u16 	%f3, %rs2;
	cvt.s64.s32 	%rd7, %r13;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u8 	%rs3, [%rd8];
	cvt.rn.f32.u16 	%f4, %rs3;
	mov.f32 	%f5, 0f3F800000;
	sub.f32 	%f6, %f5, %f2;
	mul.f32 	%f7, %f6, %f4;
	fma.rn.f32 	%f8, %f2, %f3, %f7;
	cvt.rzi.u32.f32 	%r14, %f8;
	or.b32  	%r15, %r12, 1;
	ld.global.u8 	%rs4, [%rd6+1];
	cvt.rn.f32.u16 	%f9, %rs4;
	sub.s32 	%r16, %r15, %r11;
	cvt.s64.s32 	%rd9, %r16;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.u8 	%rs5, [%rd10];
	cvt.rn.f32.u16 	%f10, %rs5;
	mul.f32 	%f11, %f6, %f10;
	fma.rn.f32 	%f12, %f2, %f9, %f11;
	cvt.rzi.u32.f32 	%r17, %f12;
	ld.global.u8 	%rs6, [%rd6+2];
	cvt.rn.f32.u16 	%f13, %rs6;
	ld.global.u8 	%rs7, [%rd10+1];
	cvt.rn.f32.u16 	%f14, %rs7;
	mul.f32 	%f15, %f6, %f14;
	fma.rn.f32 	%f16, %f2, %f13, %f15;
	cvt.rzi.u32.f32 	%r18, %f16;
	st.global.u8 	[%rd8], %r14;
	st.global.u8 	[%rd10], %r17;
	st.global.u8 	[%rd10+1], %r18;

$L__BB6_2:
	ret;

}

